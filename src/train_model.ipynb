{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3d11c8b",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea0b1fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img, array_to_img\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9d572a",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdba9bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_dir = 'dataset_redimensionado'\n",
    "aug_dir = 'dataset_aumentado'\n",
    "\n",
    "if os.path.exists(aug_dir):\n",
    "    shutil.rmtree(aug_dir)\n",
    "shutil.copytree(orig_dir, aug_dir, dirs_exist_ok=True)\n",
    "for root, dirs, files in os.walk(aug_dir):\n",
    "    for file in files:\n",
    "        os.remove(os.path.join(root, file))\n",
    "\n",
    "datagen_train = ImageDataGenerator(\n",
    "    zoom_range=0.1,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.8, 1.2],\n",
    "    fill_mode='constant',\n",
    "    cval=0\n",
    ")\n",
    "\n",
    "datagen_val_test = ImageDataGenerator(\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.9, 1.1],\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "n_aug_dict = {\n",
    "    'train': {'poluido': 2, 'nao_poluido': 9},\n",
    "    'val': {'poluido': 0, 'nao_poluido': 1},\n",
    "    'test': {'poluido': 0, 'nao_poluido': 1}\n",
    "}\n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    for classe in ['poluido', 'nao_poluido']:\n",
    "        src_folder = os.path.join(orig_dir, split, classe)\n",
    "        dst_folder = os.path.join(aug_dir, split, classe)\n",
    "        os.makedirs(dst_folder, exist_ok=True)\n",
    "        n_aug = n_aug_dict[split][classe]\n",
    "\n",
    "        if split == 'train':\n",
    "            datagen = datagen_train\n",
    "            apply_suavizar = True\n",
    "        else:\n",
    "            datagen = datagen_val_test\n",
    "            apply_suavizar = False\n",
    "\n",
    "        for fname in os.listdir(src_folder):\n",
    "            if not fname.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                continue\n",
    "            img_path = os.path.join(src_folder, fname)\n",
    "            img = load_img(img_path)\n",
    "            x = img_to_array(img)\n",
    "            x = x.reshape((1,) + x.shape)\n",
    "            array_to_img(x[0]).save(os.path.join(dst_folder, fname))\n",
    "\n",
    "            if n_aug > 0:\n",
    "                i = 0\n",
    "                for batch in datagen.flow(x, batch_size=1):\n",
    "                    new_fname = f\"{os.path.splitext(fname)[0]}_aug{i+1}.jpg\"\n",
    "                    array_to_img(batch[0]).save(os.path.join(dst_folder, new_fname))\n",
    "                    i += 1\n",
    "                    if i >= n_aug:\n",
    "                        break\n",
    "\n",
    "print(\"✅ Augmentation finalizado. Imagens salvas em:\", aug_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfb5b12",
   "metadata": {},
   "source": [
    "## Treinamento com Transfer Learning e MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10a3117",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'dataset_aumentado/train'\n",
    "val_dir = 'dataset_aumentado/val'\n",
    "test_dir = 'dataset_aumentado/test'\n",
    "\n",
    "train_gen = ImageDataGenerator(rescale=1./255)\n",
    "val_gen = ImageDataGenerator(rescale=1./255)\n",
    "test_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_data = train_gen.flow_from_directory(\n",
    "    train_dir, target_size=(224, 224), batch_size=16, class_mode='binary'\n",
    ")\n",
    "val_data = val_gen.flow_from_directory(\n",
    "    val_dir, target_size=(224, 224), batch_size=16, class_mode='binary'\n",
    ")\n",
    "test_data = test_gen.flow_from_directory(\n",
    "    test_dir, target_size=(224, 224), batch_size=16, class_mode='binary', shuffle=False\n",
    ")\n",
    "\n",
    "result_file = 'resultados_20_execucoes2.txt'\n",
    "if os.path.exists(result_file):\n",
    "    os.remove(result_file)\n",
    "\n",
    "best_f1 = 0\n",
    "\n",
    "all_acc = []\n",
    "all_val_acc = []\n",
    "all_loss = []\n",
    "all_val_loss = []\n",
    "all_conf_matrices = []\n",
    "\n",
    "print(\"\\n✅ Iniciando as 20 execuções...\\n\")\n",
    "\n",
    "# ========================\n",
    "# Execuções (Treino 20x)\n",
    "# ========================\n",
    "for exec_num in range(1, 21):\n",
    "    print(f\"\\n========== Execução {exec_num} ==========\\n\")\n",
    "\n",
    "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    preds = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=preds)\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    total_train = train_data.classes.shape[0]\n",
    "    class_0 = np.sum(train_data.classes == 0)\n",
    "    class_1 = np.sum(train_data.classes == 1)\n",
    "    weight_for_0 = (1 / class_0) * (total_train / 2.0)\n",
    "    weight_for_1 = (1 / class_1) * (total_train / 2.0)\n",
    "    class_weights = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "    history = model.fit(\n",
    "        train_data,\n",
    "        validation_data=val_data,\n",
    "        epochs=20,\n",
    "        class_weight=class_weights,\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    all_acc.append(history.history['accuracy'])\n",
    "    all_val_acc.append(history.history['val_accuracy'])\n",
    "    all_loss.append(history.history['loss'])\n",
    "    all_val_loss.append(history.history['val_loss'])\n",
    "\n",
    "    loss, acc = model.evaluate(test_data, verbose=0)\n",
    "    y_true = test_data.classes\n",
    "    y_pred_probs = model.predict(test_data, verbose=0).flatten()\n",
    "    y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "    report = classification_report(y_true, y_pred, target_names=['nao_poluido', 'poluido'])\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    all_conf_matrices.append(conf_matrix)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    with open(result_file, 'a') as f:\n",
    "        f.write(f'\\n========== Execução {exec_num} ==========\\n')\n",
    "        f.write(f'Train Accuracy: {history.history[\"accuracy\"][-1]:.4f}\\n')\n",
    "        f.write(f'Train Loss: {history.history[\"loss\"][-1]:.4f}\\n')\n",
    "        f.write(f'Val Accuracy: {history.history[\"val_accuracy\"][-1]:.4f}\\n')\n",
    "        f.write(f'Val Loss: {history.history[\"val_loss\"][-1]:.4f}\\n')\n",
    "        f.write(f'Test Accuracy: {acc:.4f}\\n')\n",
    "        f.write(f'F1 no Teste: {f1:.4f}\\n')\n",
    "        f.write('\\nClassification Report:\\n')\n",
    "        f.write(report)\n",
    "        f.write('\\nConfusion Matrix:\\n')\n",
    "        f.write(np.array2string(conf_matrix))\n",
    "        f.write('\\n\\n')\n",
    "\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        model.save('melhor_modelo.h5')\n",
    "        print(f'✅ Novo melhor modelo salvo com F1: {best_f1:.4f}')\n",
    "\n",
    "np.save('all_acc.npy', np.array(all_acc))\n",
    "np.save('all_val_acc.npy', np.array(all_val_acc))\n",
    "np.save('all_loss.npy', np.array(all_loss))\n",
    "np.save('all_val_loss.npy', np.array(all_val_loss))\n",
    "np.save('all_conf_matrices.npy', np.array(all_conf_matrices))\n",
    "\n",
    "print(\"\\n✅ As 20 execuções terminaram. Resultados salvos no arquivo:\", result_file)\n",
    "print(\"✅ Históricos de treino e matrizes de confusão salvos em arquivos .npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a4f259",
   "metadata": {},
   "source": [
    "## Calculos dos valores médios obtidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba9ae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# Parte 2 - Cálculo das médias das 20 execuções\n",
    "# ==============================================\n",
    "metrics_to_average = ['Train Accuracy', 'Train Loss', 'Val Accuracy', 'Val Loss', 'Test Accuracy', 'F1 no Teste']\n",
    "results = {metric: [] for metric in metrics_to_average}\n",
    "\n",
    "with open(result_file, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        for metric in metrics_to_average:\n",
    "            if line.startswith(metric):\n",
    "                value = float(re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", line.strip().split(':')[-1])[0])\n",
    "                results[metric].append(value)\n",
    "\n",
    "summary_file = 'resumo_final.txt'\n",
    "with open(summary_file, 'w') as f:\n",
    "    f.write(\"Médias das 20 Execuções:\\n\\n\")\n",
    "    for metric in metrics_to_average:\n",
    "        values = results[metric]\n",
    "        mean = np.mean(values)\n",
    "        std = np.std(values)\n",
    "        f.write(f'{metric}: Média = {mean:.4f} | Desvio Padrão = {std:.4f}\\n')\n",
    "\n",
    "print(\"\\n✅ Resumo final com as médias salvo em:\", summary_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30817ae4",
   "metadata": {},
   "source": [
    "## BoxPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67708c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.boxplot([results[m] for m in metrics_to_average], labels=metrics_to_average, showmeans=True)\n",
    "plt.title('Distribuição das métricas nas 20 execuções')\n",
    "plt.ylabel('Valor')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RiosPoluidos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
